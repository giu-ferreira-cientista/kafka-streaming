{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec70525a-a64d-4aad-bb57-b451559b3c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DoubleType, TimestampType \t\n",
    "\n",
    "\n",
    "\n",
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('json-aggregator')\n",
    "         # Add kafka package\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd434c42-ff2c-4154-8139-008ed268072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") # kafka server      \n",
    "  .option(\"subscribe\", \"patient-data\") # topic\n",
    "  .option(\"startingOffsets\", \"earliest\") # start from beginning\n",
    "  .load())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "355a1557-1545-4ea7-a82c-d0fc336a40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create schema for patient\n",
    "mySchema = StructType([\n",
    " StructField(\"id\", IntegerType()),\n",
    " StructField(\"nome\", StringType()),\n",
    " StructField(\"idade\", IntegerType()),\n",
    " StructField(\"sexo\", IntegerType()),\n",
    " StructField(\"peso\", DoubleType()),\n",
    " StructField(\"altura\", IntegerType()),\n",
    " StructField(\"bpm\", DoubleType()),\n",
    " StructField(\"pressao\", DoubleType()),\n",
    " StructField(\"respiracao\", DoubleType()),\n",
    " StructField(\"temperatura\", DoubleType()),\n",
    " StructField(\"glicemia\", DoubleType()),\n",
    " StructField(\"saturacao_oxigenio\", DoubleType()),\n",
    " StructField(\"estado_atividade\", IntegerType()),\n",
    " StructField(\"dia_de_semana\", IntegerType()),\n",
    " StructField(\"periodo_do_dia\", IntegerType()),\n",
    " StructField(\"semana_do_mes\", IntegerType()),\n",
    " StructField(\"estacao_do_ano\", IntegerType()),\n",
    " StructField(\"passos\", IntegerType()),\n",
    " StructField(\"calorias\", DoubleType()),\n",
    " StructField(\"distancia\", DoubleType()),\n",
    " StructField(\"tempo\", DoubleType()),\n",
    " StructField(\"total_sleep_last_24\", DoubleType()),\n",
    " StructField(\"deep_sleep_last_24\", DoubleType()),\n",
    " StructField(\"light_sleep_last_24\", DoubleType()),\n",
    " StructField(\"awake_last_24\", DoubleType()),\n",
    " StructField(\"fumante\", IntegerType()),\n",
    " StructField(\"genetica\", IntegerType()),\n",
    " StructField(\"gestante\", IntegerType()),\n",
    " StructField(\"frutas\", IntegerType()),\n",
    " StructField(\"vegetais\", IntegerType()),\n",
    " StructField(\"alcool\", IntegerType()),\n",
    " StructField(\"doenca_coracao\", IntegerType()),     \n",
    " StructField(\"avc\", IntegerType()),\n",
    " StructField(\"colesterol_alto\", IntegerType()), \n",
    " StructField(\"exercicio\", IntegerType()), \n",
    " StructField(\"timestampstr\", TimestampType()),\n",
    " StructField(\"timestamp_epoch\", StringType())       \n",
    " \n",
    "])\n",
    "\n",
    "# extract data and ensure `eventTime` is timestamp\n",
    "df = (\n",
    "    df.selectExpr(\"CAST(value as string)\")\n",
    "      .select(F.from_json(F.col(\"value\"),mySchema).alias(\"json_value\"))\n",
    "      .selectExpr(\"json_value.*\") # gives us a dataframe with columns (eventTime,temperatura, etc...)\n",
    "      .select(\n",
    "          F.expr(\"CAST(timestampstr as timestamp)\").alias(\"eventTime\"),\n",
    "          F.col(\"nome\"),\n",
    "          F.col(\"temperatura\"),\n",
    "          F.col(\"bpm\")\n",
    "      )\n",
    "      \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6237a5ee-5fd4-4416-82be-88dbb50190a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# when using window you will get a range or value resembling [start,end]. \n",
    "# I have chosen the `start` for this example\n",
    "from pyspark.sql.functions import col, window\n",
    "\n",
    "windowedAvg = ( \n",
    "    df.withWatermark(\"eventTime\", \"5 minutes\") \n",
    "      .groupBy(window(F.col(\"eventTime\"), \"5 minutes\").alias('eventTimeWindow'), F.col(\"nome\"))\n",
    "      .agg(F.avg(\"temperatura\").alias(\"avgtemperature\"),F.avg(\"bpm\").alias(\"avgbpm\"))       \n",
    "      .orderBy(F.col(\"eventTimeWindow\"))\n",
    "      .select(\n",
    "          F.col(\"eventTimeWindow.start\").alias(\"eventTime\"),\n",
    "          F.col(\"nome\"),\n",
    "          F.col(\"avgtemperature\"),\n",
    "          F.col(\"avgbpm\")\n",
    "          \n",
    "      )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "848a154f-f586-4d69-97c3-ea3f21991925",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# continue with your code to write to your various streams\n",
    "query = windowedAvg\\\n",
    "        .select(\n",
    "            F.expr(\"CAST(eventTime AS STRING)\").alias(\"key\"),\n",
    "            F.expr(\"'{\\\"eventTime\\\":\\\"' || CAST(eventTime AS STRING) || '\\\",' || '\\\"nome\\\":' || CAST(nome AS STRING) || ',' || '\\\"avgbpm\\\":' || CAST(avgbpm AS STRING) || ',' || '\\\"avgtemp\\\":' || CAST(avgtemperature AS STRING) || '}'\").alias(\"value\")            \n",
    "        ) \\\n",
    "        .writeStream\\\n",
    "        .outputMode('complete')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'true')\\\n",
    "        .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51a1febf-2f2a-4a72-8243-c2554e18ff20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "Partition patient-data-0's offset was changed from 201 to 10, some data may have been missed. \nSome data may have been lost because they are not available in Kafka any more; either the\n data was aged out by Kafka or the topic may have been deleted before all the data in the\n topic was processed. If you don't want your streaming query to fail on such cases, set the\n source option \"failOnDataLoss\" to \"false\".\n    \n=== Streaming Query ===\nIdentifier: [id = 8c7b0016-4ea7-4e8f-bb3f-9c7bbe718911, runId = ef23a8d2-61b2-429c-b2b6-1953f718971b]\nCurrent Committed Offsets: {KafkaV2[Subscribe[patient-data]]: {\"patient-data\":{\"0\":201}}}\nCurrent Available Offsets: {KafkaV2[Subscribe[patient-data]]: {\"patient-data\":{\"0\":10}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource org.apache.spark.sql.kafka010.KafkaStreamingWrite@76965309\n+- Project [cast(eventTime#119 as string) AS key#131, concat(concat(concat(concat(concat(concat(concat(concat(concat(concat(concat({\"eventTime\":\", cast(eventTime#119 as string)), \",), \"nome\":\"), nome#26), \",), \"avgbpm\":), cast(avgbpm#113 as string)), ,), \"avgtemp\":), cast(avgtemperature#111 as string)), }) AS value#132]\n   +- Project [eventTimeWindow#105-T300000ms.start AS eventTime#119, nome#26, avgtemperature#111, avgbpm#113]\n      +- Sort [eventTimeWindow#105-T300000ms ASC NULLS FIRST], true\n         +- Aggregate [window#114-T300000ms, nome#26], [window#114-T300000ms AS eventTimeWindow#105-T300000ms, nome#26, avg(temperatura#34) AS avgtemperature#111, avg(bpm#31) AS avgbpm#113]\n            +- Filter isnotnull(eventTime#99-T300000ms)\n               +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(eventTime#99-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(eventTime#99-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(eventTime#99-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(eventTime#99-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(eventTime#99-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(eventTime#99-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(eventTime#99-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(eventTime#99-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0) + 300000000), LongType, TimestampType)) AS window#114-T300000ms, eventTime#99-T300000ms, nome#26, temperatura#34, bpm#31]\n                  +- EventTimeWatermark eventTime#99: timestamp, 5 minutes\n                     +- Project [cast(timestampstr#60 as timestamp) AS eventTime#99, nome#26, temperatura#34, bpm#31]\n                        +- Project [json_value#23.id AS id#25, json_value#23.nome AS nome#26, json_value#23.idade AS idade#27, json_value#23.sexo AS sexo#28, json_value#23.peso AS peso#29, json_value#23.altura AS altura#30, json_value#23.bpm AS bpm#31, json_value#23.pressao AS pressao#32, json_value#23.respiracao AS respiracao#33, json_value#23.temperatura AS temperatura#34, json_value#23.glicemia AS glicemia#35, json_value#23.saturacao_oxigenio AS saturacao_oxigenio#36, json_value#23.estado_atividade AS estado_atividade#37, json_value#23.dia_de_semana AS dia_de_semana#38, json_value#23.periodo_do_dia AS periodo_do_dia#39, json_value#23.semana_do_mes AS semana_do_mes#40, json_value#23.estacao_do_ano AS estacao_do_ano#41, json_value#23.passos AS passos#42, json_value#23.calorias AS calorias#43, json_value#23.distancia AS distancia#44, json_value#23.tempo AS tempo#45, json_value#23.total_sleep_last_24 AS total_sleep_last_24#46, json_value#23.deep_sleep_last_24 AS deep_sleep_last_24#47, json_value#23.light_sleep_last_24 AS light_sleep_last_24#48, ... 13 more fields]\n                           +- Project [from_json(StructField(id,IntegerType,true), StructField(nome,StringType,true), StructField(idade,IntegerType,true), StructField(sexo,IntegerType,true), StructField(peso,DoubleType,true), StructField(altura,IntegerType,true), StructField(bpm,DoubleType,true), StructField(pressao,DoubleType,true), StructField(respiracao,DoubleType,true), StructField(temperatura,DoubleType,true), StructField(glicemia,DoubleType,true), StructField(saturacao_oxigenio,DoubleType,true), StructField(estado_atividade,IntegerType,true), StructField(dia_de_semana,IntegerType,true), StructField(periodo_do_dia,IntegerType,true), StructField(semana_do_mes,IntegerType,true), StructField(estacao_do_ano,IntegerType,true), StructField(passos,IntegerType,true), StructField(calorias,DoubleType,true), StructField(distancia,DoubleType,true), StructField(tempo,DoubleType,true), StructField(total_sleep_last_24,DoubleType,true), StructField(deep_sleep_last_24,DoubleType,true), StructField(light_sleep_last_24,DoubleType,true), ... 15 more fields) AS json_value#23]\n                              +- Project [cast(value#8 as string) AS value#21]\n                                 +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@6d2dd8ae, KafkaV2[Subscribe[patient-data]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b2f980de10c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# write on kafka topic avgtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# here i've chosen as an example to use the eventTime as the key and the value to be the avgtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m qk = (windowedAvg \n\u001b[0m\u001b[1;32m      4\u001b[0m         .select(\n\u001b[1;32m      5\u001b[0m             \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CAST(eventTime AS STRING)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: Partition patient-data-0's offset was changed from 201 to 10, some data may have been missed. \nSome data may have been lost because they are not available in Kafka any more; either the\n data was aged out by Kafka or the topic may have been deleted before all the data in the\n topic was processed. If you don't want your streaming query to fail on such cases, set the\n source option \"failOnDataLoss\" to \"false\".\n    \n=== Streaming Query ===\nIdentifier: [id = 8c7b0016-4ea7-4e8f-bb3f-9c7bbe718911, runId = ef23a8d2-61b2-429c-b2b6-1953f718971b]\nCurrent Committed Offsets: {KafkaV2[Subscribe[patient-data]]: {\"patient-data\":{\"0\":201}}}\nCurrent Available Offsets: {KafkaV2[Subscribe[patient-data]]: {\"patient-data\":{\"0\":10}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource org.apache.spark.sql.kafka010.KafkaStreamingWrite@76965309\n+- Project [cast(eventTime#119 as string) AS key#131, concat(concat(concat(concat(concat(concat(concat(concat(concat(concat(concat({\"eventTime\":\", cast(eventTime#119 as string)), \",), \"nome\":\"), nome#26), \",), \"avgbpm\":), cast(avgbpm#113 as string)), ,), \"avgtemp\":), cast(avgtemperature#111 as string)), }) AS value#132]\n   +- Project [eventTimeWindow#105-T300000ms.start AS eventTime#119, nome#26, avgtemperature#111, avgbpm#113]\n      +- Sort [eventTimeWindow#105-T300000ms ASC NULLS FIRST], true\n         +- Aggregate [window#114-T300000ms, nome#26], [window#114-T300000ms AS eventTimeWindow#105-T300000ms, nome#26, avg(temperatura#34) AS avgtemperature#111, avg(bpm#31) AS avgbpm#113]\n            +- Filter isnotnull(eventTime#99-T300000ms)\n               +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(eventTime#99-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(eventTime#99-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(eventTime#99-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(eventTime#99-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(eventTime#99-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(eventTime#99-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(eventTime#99-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(eventTime#99-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0) + 300000000), LongType, TimestampType)) AS window#114-T300000ms, eventTime#99-T300000ms, nome#26, temperatura#34, bpm#31]\n                  +- EventTimeWatermark eventTime#99: timestamp, 5 minutes\n                     +- Project [cast(timestampstr#60 as timestamp) AS eventTime#99, nome#26, temperatura#34, bpm#31]\n                        +- Project [json_value#23.id AS id#25, json_value#23.nome AS nome#26, json_value#23.idade AS idade#27, json_value#23.sexo AS sexo#28, json_value#23.peso AS peso#29, json_value#23.altura AS altura#30, json_value#23.bpm AS bpm#31, json_value#23.pressao AS pressao#32, json_value#23.respiracao AS respiracao#33, json_value#23.temperatura AS temperatura#34, json_value#23.glicemia AS glicemia#35, json_value#23.saturacao_oxigenio AS saturacao_oxigenio#36, json_value#23.estado_atividade AS estado_atividade#37, json_value#23.dia_de_semana AS dia_de_semana#38, json_value#23.periodo_do_dia AS periodo_do_dia#39, json_value#23.semana_do_mes AS semana_do_mes#40, json_value#23.estacao_do_ano AS estacao_do_ano#41, json_value#23.passos AS passos#42, json_value#23.calorias AS calorias#43, json_value#23.distancia AS distancia#44, json_value#23.tempo AS tempo#45, json_value#23.total_sleep_last_24 AS total_sleep_last_24#46, json_value#23.deep_sleep_last_24 AS deep_sleep_last_24#47, json_value#23.light_sleep_last_24 AS light_sleep_last_24#48, ... 13 more fields]\n                           +- Project [from_json(StructField(id,IntegerType,true), StructField(nome,StringType,true), StructField(idade,IntegerType,true), StructField(sexo,IntegerType,true), StructField(peso,DoubleType,true), StructField(altura,IntegerType,true), StructField(bpm,DoubleType,true), StructField(pressao,DoubleType,true), StructField(respiracao,DoubleType,true), StructField(temperatura,DoubleType,true), StructField(glicemia,DoubleType,true), StructField(saturacao_oxigenio,DoubleType,true), StructField(estado_atividade,IntegerType,true), StructField(dia_de_semana,IntegerType,true), StructField(periodo_do_dia,IntegerType,true), StructField(semana_do_mes,IntegerType,true), StructField(estacao_do_ano,IntegerType,true), StructField(passos,IntegerType,true), StructField(calorias,DoubleType,true), StructField(distancia,DoubleType,true), StructField(tempo,DoubleType,true), StructField(total_sleep_last_24,DoubleType,true), StructField(deep_sleep_last_24,DoubleType,true), StructField(light_sleep_last_24,DoubleType,true), ... 15 more fields) AS json_value#23]\n                              +- Project [cast(value#8 as string) AS value#21]\n                                 +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@6d2dd8ae, KafkaV2[Subscribe[patient-data]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# write on kafka topic avgtemperature\n",
    "# here i've chosen as an example to use the eventTime as the key and the value to be the avgtemperature\n",
    "qk = (windowedAvg \n",
    "        .select(\n",
    "            F.expr(\"CAST(eventTime AS STRING)\").alias(\"key\"),\n",
    "            F.expr(\"'{\\\"eventTime\\\":\\\"' || CAST(eventTime AS STRING) || '\\\",' || '\\\"nome\\\":\\\"' || nome || '\\\",' || '\\\"avgbpm\\\":' || CAST(avgbpm AS STRING) || ',' || '\\\"avgtemp\\\":' || CAST(avgtemperature AS STRING) || '}'\").alias(\"value\")            \n",
    "        ) \n",
    "        .writeStream \n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \n",
    "        .option(\"checkpointLocation\", \"/home/jovyan/work/json/checkpoint\") \n",
    "        .option(\"topic\", \"avg-data\")        \n",
    "        .outputMode(\"complete\") \n",
    "        .start()\n",
    "        .awaitTermination())\n",
    "        #query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73f222-700e-4dd6-9a20-93b0724916a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a202df85-a817-4738-ba5c-ac0367d92583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489f84ff-b312-4bfe-bf64-9f692507a444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9ac03a0a6051494cc606d484d27d20fce22fb7b4d169f583271e11d5ba46a56e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
